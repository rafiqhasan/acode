{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Developed by Hasan \n",
    "#python example to infer document vectors from trained doc2vec model - jhlau\n",
    "import gensim.models as g\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrafiq\\Documents\\HRafiq_Deloitte\\FIRM_Initiatives\\AutoCode\\Doc2Vec pretrained\\model\\doc2vec.bin\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "#First download from: https://ibm.box.com/s/77etivy69jmga0x0u6vs2n47ul8baks4\n",
    "gen_model = os.getcwd() + \"\\Doc2Vec pretrained\\model\\doc2vec.bin\"\n",
    "print(gen_model)\n",
    "m = g.Doc2Vec.load(gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read FS 1\n",
    "# text = textract.process(\"FS_example.docx\").decode(\"utf-8\")\n",
    "\n",
    "# #Name of start section header:\n",
    "# start_header = '\\nDetailed Description of Enhancement \\n'\n",
    "# end_header = '\\nAccess Method or Path \\n'\n",
    "# start_pos = text.find(start_header) + len(start_header)\n",
    "# end_pos = text.find(end_header)\n",
    "\n",
    "# #Get section text\n",
    "# section_text1 = text[start_pos:end_pos].rstrip()\n",
    "section_text1 = \"In the to-be business process the RAR contracts will be created if and only if the operational documents (sales orders / sales contracts / service contracts) are in “Ready for RAR” status. Additionally if all the pre-requisites of contract combine are met and the operational document in process is in “Ready for RAR” status and it corresponds to the same “IFRS 15 ID” as an existing operational document with RAR contract already, the POBs of the new document will be combined with the existing RAR contract .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read FS 2\n",
    "# text = textract.process(\"FS_example.docx\").decode(\"utf-8\")\n",
    "\n",
    "# #Name of start section header:\n",
    "# start_header = '\\nDetailed Description of Enhancement \\n'\n",
    "# end_header = '\\nAccess Method or Path \\n'\n",
    "# start_pos = text.find(start_header) + len(start_header)\n",
    "# end_pos = text.find(end_header)\n",
    "\n",
    "# #Get section text\n",
    "# section_text1 = text[start_pos:end_pos].rstrip()\n",
    "section_text2 = \"The adjustments are necessary to fulfill the requirements of Healthcare and here in the special kind of Invitro business. A manual solution is not possible, because of the high volume of this business. The SAP Standard cannot fulfill all requirements, so that this enhancement is needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_text3 = \"This is simply a dummy text with no match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(v_sent):\n",
    "    #Tokenize all words\n",
    "    tokens = nltk.word_tokenize(v_sent)\n",
    "    \n",
    "    #Lower case all words\n",
    "    word_tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    #Remove english stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #Remove stop words\n",
    "    filtered_tokens = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    #print(filtered_tokens)\n",
    "    #print(\"\\n\")\n",
    "    return(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Document to Vector rep\n",
    "v1 = m.infer_vector(preprocess_text(section_text1), alpha=start_alpha, steps=infer_epoch).reshape(1,-1)\n",
    "v2 = m.infer_vector(preprocess_text(section_text2), alpha=start_alpha, steps=infer_epoch).reshape(1,-1)\n",
    "v3 = m.infer_vector(preprocess_text(section_text3), alpha=start_alpha, steps=infer_epoch).reshape(1,-1)\n",
    "doc_store = np.concatenate([v2,v3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim1: [[0.5662714  0.38311413]]\n"
     ]
    }
   ],
   "source": [
    "#Get document similarity\n",
    "print(\"Sim1:\", cosine_similarity(v1, doc_store))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
