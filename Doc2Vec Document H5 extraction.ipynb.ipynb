{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import h5py\n",
    "import multiprocessing\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import os\n",
    "\n",
    "from flask import Flask, render_template, request, redirect, url_for, jsonify, make_response\n",
    "from werkzeug.utils import secure_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_process(word_document):    \n",
    "    fs = [docx2txt.process(word_document)]\n",
    "    \n",
    "    data = [' '.join(i.split()) for i in fs] #strip white spaces\n",
    "\n",
    "    data1 = [re.sub(r'[^\\w\\s]',' ',i) for i in data] #removes punctuation\n",
    "\n",
    "    data2 = []\n",
    "\n",
    "    for j in range(0,len(data1)):\n",
    "        data2.append(''.join(i for i in data1[j] if i.isdigit() == False)) #removes numbers\n",
    "\n",
    "    data3 = [' '.join(i.split()) for i in data2]\n",
    "\n",
    "    gen_docs = [[w.lower() for w in word_tokenize(text)] for text in data3] #tokenize each document\n",
    "\n",
    "    gendocs1 = []\n",
    "    for i in range(0,len(gen_docs)):\n",
    "        gendocs1.append([w for w in gen_docs[i] if not w in stop_words]) #removes stop words\n",
    "\n",
    "    txt_fnlwrds = []\n",
    "    for j in range(0,len(data3)):\n",
    "        txt_fnlwrds.append([i for i in gendocs1[j] if len(i) >= 3 ]) #removes words which have <3 letters in it\n",
    "\n",
    "    return txt_fnlwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading pre-trained model\n",
    "gen_model = os.getcwd() + \"\\Doc2Vec pretrained\\model\\doc2vec.bin\"\n",
    "model = gensim.models.Doc2Vec.load(gen_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a H5 file for file names or labels\n",
    "\n",
    "dummyLabel = [\"Dummy\"]\n",
    "labelVec = np.array(dummyLabel, dtype=h5py.special_dtype(vlen=str))\n",
    "\n",
    "label_h5f = h5py.File('label_h5f.h5', 'w')\n",
    "label_h5f.create_dataset('labelDataset', data =labelVec, maxshape=(None,), chunks=True)\n",
    "label_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_h5f = h5py.File('label_h5f.h5','r')\n",
    "existLabel = label_h5f['labelDataset'][:]\n",
    "label_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a H5 file for document vectors\n",
    "\n",
    "dummyTxt = \"This is dummy text to initiate H5 File\"\n",
    "dummyVec = model.infer_vector(dummyTxt.split(), alpha=start_alpha, steps=infer_epoch).reshape(1,-1)\n",
    "\n",
    "vec_h5f = h5py.File('vec_h5f.h5', 'w')\n",
    "vec_h5f.create_dataset('vecDataset', data = dummyVec, maxshape=(None, None), chunks=True)\n",
    "vec_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_h5f = h5py.File('vec_h5f.h5','r')\n",
    "existVec = vec_h5f['vecDataset'][:]\n",
    "vec_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [26/Jun/2018 11:36:06] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [26/Jun/2018 11:36:13] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__, template_folder='C:/Users/gkottur/Documents/FI/AutoCodeFS/Scripts/')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload():\n",
    "    file = request.files['file']\n",
    "    newLabel = np.array([file.filename], dtype=h5py.special_dtype(vlen=str))\n",
    "    #Appends the file name of the file uploaded to label_h5f.h5\n",
    "    with h5py.File('label_h5f.h5', 'r+') as label:\n",
    "        label[\"labelDataset\"].resize((label[\"labelDataset\"].shape[0] + newLabel.shape[0]), axis = 0)\n",
    "        label[\"labelDataset\"][-newLabel.shape[0]:] = newLabel\n",
    "    \n",
    "    docPreproc = txt_process(file)\n",
    "    docPreproc1 = [item for sublist in docPreproc for item in sublist]\n",
    "    newVec = model.infer_vector(docPreproc1, alpha=start_alpha, steps=infer_epoch).reshape(1,-1)\n",
    "    #Appends the vector of the uploaded document to vec_h5f.h5\n",
    "    with h5py.File('vec_h5f.h5', 'r+') as vec:\n",
    "        vec[\"vecDataset\"].resize((vec[\"vecDataset\"].shape[0] + newVec.shape[0]), axis = 0)\n",
    "        vec[\"vecDataset\"][-newVec.shape[0]:] = newVec\n",
    "        \n",
    "    return \"H5 successfully appended\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='localhost', port=5000, threaded=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
